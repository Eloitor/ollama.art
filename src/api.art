===============================================
; Ollama.art
;
; Ollama API wrapper
; for Arturo
;
; MIT License
; (c) 2024 Yanis Zafirópulos
; (c) 2024 Eloi Torrents
;==========================================================
; @file: src/api.art
;==========================================================

import 'recase!

;--------------------------
; Type Definitions 
;--------------------------

define :message [
    init: method [role :string content :string][
        \role: role
        \content: content

        if not? in? \role ["system" "user" "assistant" "tool"] ->
            throw.as: valueError "Message role must be 'system', 'user', 'assistant' or 'tool'"
    ]
]

define :embedding [
    init: method [response][
        unless key? response 'version ->
            throw "Something went wrong"
        \version: response\version
        unless key? response 'body ->
            throw "Something went wrong"
        \embedding: get read.json get response 'body 'embedding
    ]
]

;--------------------------
; Helper Functions
;--------------------------

createMessage: function [role :string content :string][
    ;; description: « Create a new message object
    ;; returns: :message
    to :message [role content]
]

;--------------------------
; Main implementation
;--------------------------

define :ollamaAPI [
    init: method [][
        \host: (attr 'host) ?? "http://localhost:11434"
        \model: "llama3.2"
        if null? request \host ø
            -> panic "Ollama is not running!"
    ]

    chat: method [blk :block][
        ;; description: « Send a chat request to Ollama's API
        ;; options: [
        ;;     model: :string « specify the model to use (e.g., llama3.2)
        ;;     system: :string « system instuctions
        ;;     tools: [:string] « descriptions of available tools
        ;;     numKeep: :integer « number of tokens to retain from the initial prompt
        ;;     seed: :integer « sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. (Default: 0)
        ;;     numPredict: :integer « maximum number of tokens to predict when generating text. (Default: 128, -1 = infinite generation, -2 = fill context)
        ;;     topK: :integer « reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)
        ;;     topP: :floating « works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)
        ;;     minP: :floating « alternative to the top_p, and aims to ensure a balance of quality and variety. The parameter p represents the minimum probability for a token to be considered, relative to the probability of the most likely token. For example, with p=0.05 and the most likely token having a probability of 0.9, logits with a value less than 0.045 are filtered out. (Default: 0.0)
        ;;     tfsZ: :floating « tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. (default: 1)
        ;;     typicalP: :floating « locally typical sampling, parameter p (default: 1.0)
        ;;     repeatLastN: :integer « sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)
        ;;     temperature: :floating « the temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)
        ;;     repeatPenalty: :floating « sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)
        ;;     presencePenalty: :floating « penalize tokens already present (default: 0.0)
        ;;     frequencyPenalty: :floating « penalize frequent tokens (default: 0.0)
        ;;     mirostat: :integer « enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)
        ;;     mirostatEta: :floating « influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. (Default: 0.1)
        ;;     mirostatTau: :floating « controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0)
        ;;     penalizeNewline: :boolean « penalize generation of newline tokens (default: true)
        ;;     stop: [:string] « sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return.  (e.g., ["\n", "user:"])
        ;;     numa: :boolean « enable NUMA optimization (default: false)
        ;;     numCtx: :integer « sets the size of the context window used to generate the next token. (Default: 2048)
        ;;     numBatch: :integer « number of tokens in the prompt that are fed into the model at a time.
        ;;     numGpu: :integer « number of layers to send to the GPU
        ;;     mainGpu: :integer « primary GPU index for processing (default: 0)
        ;;     lowVram: :boolean « optimize for low GPU memory (default: false)
        ;;     vocabOnly: :boolean « limit operations to vocabulary-related tasks (default: false)
        ;;     useMmap: :boolean « use memory-mapped files for data access (default: true)
        ;;     useMlock: :boolean « lock model in RAM to prevent swapping (default: false)
        ;;     numThread: :integer « number of CPU threads to use (default: system threads)
        ;; ]
        ;; returns: :dictionary
        ;; example: {
        ;;     ollama: to :ollamaAPI [ø]!
        ;;     result: ollama\chat [
        ;;         to :message ["user" "What is 2+2?"]
        ;;     ]
        ;;     print result\content
        ;; }

        if null? request \host ø
            -> panic "Ollama is not running!"

        ; Get parameters from attributes or defaults
        model: (attr 'model) ?? \model
        stream: (attr 'stream) ?? false
        system: attr 'system
        tools: attr 'tools

        messages: @ blk
        ensure -> is? [:message] messages

        options: #.raw flatten select map to [:string] [
            numKeep
            seed
            numPredict
            topK
            topP
            minP
            tfsZ
            typicalP
            repeatLastN
            temperature
            repeatPenalty
            presencePenalty
            frequencyPenalty
            mirostat
            mirostatEta
            mirostatTau
            penalizeNewline
            stop
            numa
            numCtx
            numBatch
            numGpu
            mainGpu
            lowVram
            vocabOnly
            useMmap
            useMlock
            numThread
        ] 
            'at -> @[recase.snake at attr at] 
            'p -> not? null? last p

        ; Build request body
        body: #[
            model: model
            messages: messages
            stream: stream
            options: options
            tools: tools
        ]

        if not? null? system [
            set body 'system system
        ]

        ; Make API request
        result: request.post 
            .json
            \host ++ "/api/chat"
            body

        if result\status <> 200 [
            throw.as: runtimeError join @[
                "API request failed with status" 
                result\status ":" 
                read.json result\body
            ]
        ]

        return read.json result\body
    ]

    ask: method [question :string][
        ;; description: « Ask question directly to Ollama API and get answer
        ;; returns: :string
        ;; example: {
        ;;     ollama: to :ollamaAPI [ø]!
        ;;     print ollama\ask "What is the capital of Sweden?"
        ;;     ; The capital of Sweden is Stockholm.
        ;; }
        
        at: flatten.once map attrs [k v] [to :block render.once ".|k|:@|as.code v|" ]
        do at ; restore attributes
        
        messages: [
            createMessage "user" question
        ]

        until [
            response: \chat messages

            unless key? response 'message ->
                throw "Something went wrong"

            message: response\message

            unless key? message 'content ->
                throw "Something went wrong"

            if key? message 'tool_calls [
                if one? size message\tool_calls [
                    if "nop" = message\tool_calls\[0]\function\name [
                        ; remove the .tools: attr and call \chat again
                        do at ; restore attributes
                        discard: attr 'tools
                        continue
                    ] 
                ]
                loop message\tool_calls 't [
                    inspect t\function
                    toolInfo: to :string t\function
                    toolAns: input "Provide info to the LLM: "
                    'messages ++ [
                        createMessage "tool" toolInfo ++ toolAns
                    ]
                ]
                ; proccess each tool, append the messages and call chat again.
                ; remove the .tools: attr and call \chat again
                do at ; restore attributes
                discard: attr 'tools
                continue
            ]
        ] [not? key? message 'tool_calls]
        return message\content
    ]

    embeddings: method [prompt :string][
        ;; description: « Embed text into a vector space
        ;; returns: [:floating]
        ;; example: {
        ;;     ollama: to :ollamaAPI [ø]!
        ;;     result: ollama\embedngs "Arturo is a programming language"
        ;;     print result\content
        ;; }

        if null? request \host ø
            -> panic "Ollama is not running!"

        ; Get parameters from attributes or defaults
        model: (attr 'model) ?? \model

        ; Build request body
        body: #[
            model: model
            prompt: prompt
        ]

        ; Make API request
        response: request.post 
            .json
            \host ++ "/api/embeddings"
            body
        return to :embedding @[response]
    ]

    setModel: method [model :string][
        ;; description: « Set default model
        \model: model
    ]
]
